[00:00:00] SPEAKER_00: perspective on AI infrastructure and computing strategies.
[00:00:06] SPEAKER_00: And I have several specific questions about hardware architecture and also your resource allocation.
[00:00:15] SPEAKER_00: Also the recent developments and also the debates in the industry.
[00:00:20] SPEAKER_00: Are you ready to take questions?
[00:00:24] SPEAKER_01: Yes, I'm ready.
[00:00:25] SPEAKER_00: Okay, good.
[00:00:27] SPEAKER_00: So the first question is about the
[00:00:31] SPEAKER_00: the scaling law and the approach, there has been a lot of debates recently.
[00:00:36] SPEAKER_00: So with the reported diminishing returns in pre-training and
[00:00:44] SPEAKER_00: People are talking about monthly infrastructure costs around $100 million for large clusters.
[00:00:52] SPEAKER_00: I'm curious about your view and how is Microsoft re-evaluating its investment strategy between the traditional large-language model scaling versus exploring alternative scaling factors?
[00:01:12] SPEAKER_01: Yeah.
[00:01:14] SPEAKER_01: Well, okay, so the GPT, the OpenAI-01 and the successive model is clearly the direction of the future, right?
[00:01:26] SPEAKER_01: I think Microsoft would like to develop those type of model.
[00:01:34] SPEAKER_01: And then, look, we have to look at that there's a market is bifurcated, right?
[00:01:40] SPEAKER_01: It's not the,
[00:01:46] SPEAKER_01: So there's not a uniform demand for all models.
[00:01:51] SPEAKER_01: So for call center as a service, for basic Chantbot CRM, you may be okay with just Lama 3, 7 dB, Lama 3.1, 7 dB.
[00:02:00] SPEAKER_01: You don't even need the 405B.
[00:02:08] SPEAKER_01: for the enterprise workload, that is the chip design, the content generation, the detail, like the code generation, you know, the language action model,
[00:02:29] SPEAKER_01: the electronic medical records, you need high level of accuracy, the legal search, you need high level of accuracy and proficiency.
[00:02:37] SPEAKER_01: But there's also like you can, those people are willing to pay more for that model because you're replacing the human.
[00:02:44] SPEAKER_01: You know, you're basically doing a lot of labor cost savings.
[00:02:48] SPEAKER_01: So they are willing to invest, now that needs more than just the GPU of the same model, you need the better model with the improved accuracy.
[00:02:59] SPEAKER_01: So that's, I think, the direction.
[00:03:01] SPEAKER_01: So for low-pricing, just the low-wage human, you can use the existing model so long as the parameter count is less than 100 billion and scale the cluster.
[00:03:14] SPEAKER_01: But if you want to do higher-end workload, you need high accuracy, you need to develop the better model.
[00:03:19] SPEAKER_01: It's not a question of putting more GPU to run the existing model today.
[00:03:31] SPEAKER_00: Just to make sure I understand correctly, you still believe in better models, which is higher count of parameters will lead to better accuracy and better capabilities of the model?
[00:03:52] SPEAKER_01: Yeah, I do believe in, so when we say training, I do believe you will see model
[00:04:00] SPEAKER_01: that can have three trillion parameter.
[00:04:04] SPEAKER_01: I don't know how much beyond three trillion you will increase the parameters count.
[00:04:10] SPEAKER_01: I think that's probably the maximum and you have to get the best performance model you can with that limit.
[00:04:21] SPEAKER_00: Are we seeing the data, the saving of the data that prevents
[00:04:27] SPEAKER_00: way from scaling beyond 3 trillion parameters?
[00:04:33] SPEAKER_01: Yeah, basically it's cost.
[00:04:35] SPEAKER_01: The cost of the GPU will be prohibitive, right?
[00:04:43] SPEAKER_01: The GPU will be, the number of GPU you need and the scaling of the cluster will not be allowing to do parameter count more than 3 trillion model even with mixture of experts.
[00:05:00] SPEAKER_00: What if the mass deployment of NVIDIA GP200 is happening now and the cost curve with GP200 will drive the cost maybe 10x cheaper?
[00:05:16] SPEAKER_00: And would that change the dynamics?
[00:05:22] SPEAKER_01: Yeah, if the...
[00:05:26] SPEAKER_01: Yeah, if the GB200 is 10x cheaper, you still have the cost of the power.
[00:05:32] SPEAKER_01: So I think the up-front CapExcel, okay, let's say, we can do the math, right?
[00:05:37] SPEAKER_01: The cost of the GPU, maybe, like, well, Microsoft may order 200,000 GPU, and they're paying, say, 40K, or it's like 30, actually 30, 30K per GPU, okay?
[00:05:51] SPEAKER_01: This is 200,000 just this quarter for the Q1.
[00:05:55] SPEAKER_01: For the whole year, they may order like 600,000 B200s, 600 to 800.
[00:05:57] SPEAKER_01: Sorry, for 2024 or 2025?
[00:05:57] SPEAKER_01: So for 2025, the total quantity of the B200 is 800K.
[00:06:00] SPEAKER_00: 200K is what the order is placed this quarter for the delivery of first quarter next year, prior to the Chinese New Year.
[00:06:20] SPEAKER_00: I see.
[00:06:22] SPEAKER_01: Okay, so, but then what is the cost of the electricity, right?
[00:06:27] SPEAKER_01: So that's 1250 watt per GPU, and then it's, you can do it 7.5, 7.1 cents per kilowatt hour.
[00:06:38] SPEAKER_01: So you can see, like, you know, the cost of the electricity becomes more than the cost of the GPU after, like, not even one year, something like this.
[00:06:51] SPEAKER_01: assuming like 93% utilization.
[00:06:54] SPEAKER_01: So the cost of the power also becomes expensive, not just the CapEx.
[00:07:06] SPEAKER_00: I see.
[00:07:06] SPEAKER_00: So what
[00:07:15] SPEAKER_00: Is there any alternative way, like maybe some evolution of the power structure or the cooling solutions that could make the power cheaper?
[00:07:33] SPEAKER_01: Yeah, I mean, I think that the power, people are looking at nuclear and other sources to have renewable power, right?
[00:07:40] SPEAKER_01: And low carbon power.
[00:07:43] SPEAKER_01: In terms of like the cooling, unfortunately the cooling you can't do any cheaper.
[00:07:47] SPEAKER_01: If you want higher and higher power density, you have to even do immersion cooling.
[00:08:00] SPEAKER_00: Yes.
[00:08:06] SPEAKER_00: So I also have a question, like you just mentioned O1 preview, and the O1 training involved heavy race for learning compute, right?
[00:08:19] SPEAKER_00: So I'm curious about the, in terms of infrastructure, in Microsoft's infrastructure planning, like,
[00:08:29] SPEAKER_00: what is the difference between the cluster designs between traditional pre-training workloads versus this new domain-specific reinforcement learning training patterns?
[00:08:44] SPEAKER_01: So if you're talking about reinforcement learning where the model is being trained by itself or model training model, you mean not RLHF, you mean not with human feedback, right?
[00:08:57] SPEAKER_00: Yes.
[00:08:59] SPEAKER_01: Okay, yeah, that's like what Meta is pioneering, and I think if you see how much percent of the GPU is used for training, today it's probably like 40% inference, 60% training.
[00:09:16] SPEAKER_01: In four years, it's going to be like probably 25% training, 75% inference.
[00:09:23] SPEAKER_01: So training is the smaller and smaller workload, so inference is actually the challenge.
[00:09:32] SPEAKER_01: So yes, I do agree that the reinforcement learning is going to be the way forward, but I don't think that helps you too much on saving your GPU.
[00:09:45] SPEAKER_00: Which one is more compute-intense, like pre-training or post-training, like RL?
[00:09:56] SPEAKER_01: Clearly the pre-training is more compute-intensive, but you don't have to do that over and over again, right?
[00:10:05] SPEAKER_01: You may do base training once and then you just do fine-tuning.
[00:10:10] SPEAKER_00: Correct.
[00:10:13] SPEAKER_00: But there's also an argument that O1 is the inferencing part.
[00:10:20] SPEAKER_00: They are doing the self-play and will generate a lot of reasoning tokens and they will reuse that.
[00:10:26] SPEAKER_00: those reasoning tokens into the post-training part.
[00:10:30] SPEAKER_00: So although the parameters count is not growing that big, but the reasoning tokens, the data set is growing maybe 10x per year.
[00:10:41] SPEAKER_00: So that will drive the compute intensity for the post-training.
[00:10:49] SPEAKER_00: Do you agree with this argument?
[00:10:54] SPEAKER_01: Yeah, I would agree.
[00:10:56] SPEAKER_01: Yeah, using the tokens generated for the Pulse training.
[00:11:03] SPEAKER_01: But they already do that today, right?
[00:11:05] SPEAKER_01: They already do that to some extent.
[00:11:08] SPEAKER_01: But I think in terms of what percent of the layers are back propagating and readjusting, it's not like, it may be 10%.
[00:11:20] SPEAKER_01: It's limited to 10 to 15%.
[00:11:24] SPEAKER_01: in one sequence, or basically in a month.
[00:11:29] SPEAKER_01: It's not continually doing the whole model again and again, unless it's being repurposed for something else, right?
[00:11:40] SPEAKER_01: So I think that the GPU capacity for inference
[00:11:44] SPEAKER_01: is not the same as just regular inference today.
[00:11:47] SPEAKER_01: If you do it for reinforcement learning, like you must probably put an additional usage factor of like 20 to 30 percent.
[00:11:53] SPEAKER_01: But I don't think that even then the traditional pre-training workload is still more compute intensive.
[00:12:08] SPEAKER_00: Right, right.
[00:12:13] SPEAKER_00: So when inferencing is accounting like 60 or 70% of the total workloads, what do you think about the cluster?
[00:12:28] SPEAKER_00: Will this change the current architecture?
[00:12:33] SPEAKER_00: For example, now we're seeing like 10, 100K or 100K
[00:12:41] SPEAKER_00: GPU cluster that, do you think the clusters are growing bigger or will we see like smaller more, like smaller clusters and also the networking solutions for inferencing and training, what's the difference?
[00:13:00] SPEAKER_01: Yeah, I mean, if you're going to talk about the networking side, so, okay, so we need to talk about how many parameters the model has.
[00:13:12] SPEAKER_01: The model may have, you know, if it's less than $70 billion, it's very straightforward, right?
[00:13:22] SPEAKER_01: Even $400 billion, I think you could argue, is easier to do, depending on how many transactions a second.
[00:13:29] SPEAKER_01: You know, InfiniBand, the NVIDIA Sharp protocol, you have Ultra Ethernet,
[00:13:38] SPEAKER_01: So you can do a cluster of like 20K GPU today with the Quantum II InfiniBand Switch and the CX8 NIC card, you know, with the... and the NVSwitch ecosystem.
[00:13:59] SPEAKER_01: If you want to do 100K GPUs, you probably cannot do that, that's too much.
[00:14:04] SPEAKER_01: That's going to require several
[00:14:07] SPEAKER_01: That means several Quantum II InfiniBand switches, pools of GPU, racks of GPU connect together, but even though that's one cluster, you may consider it as a cluster, they're not all doing the same job.
[00:14:22] SPEAKER_01: It would be hard to distribute the work to all the GPU in there.
[00:14:28] SPEAKER_01: unless it's running inference, but multiple requests of the same model.
[00:14:32] SPEAKER_01: For example, for training, I believe the largest cluster that NVIDIA has done with the B200 is like 20K.
[00:14:44] SPEAKER_01: They have papers published with 100K, but that's not, it actually, you get diminishing returns
[00:14:51] SPEAKER_01: as you add more GPU because you have HBM memory error or like, you know, some other like link error, or you may have like, you may, it takes more time for distributing the work for the GPU than it is to have fewer GPUs or the GPUs idle more.
[00:15:17] SPEAKER_00: Thank you for mentioning that.
[00:15:19] SPEAKER_00: So,
[00:15:23] SPEAKER_00: Considering the diminishing return when the clusters become bigger and bigger, do you think distributed smaller clusters would be a good choice?
[00:15:40] SPEAKER_01: Distributed smaller clusters is probably, for inference, is definitely the wise choice.
[00:15:45] SPEAKER_01: For training, I think you still need 20k.
[00:15:48] SPEAKER_01: You may not need 100, but you need 20-25k for sure.
[00:15:55] SPEAKER_00: I see.
[00:15:56] SPEAKER_00: For the inferencing part, do you prefer Ethernet over InfiniBand?
[00:16:03] SPEAKER_01: Yes.
[00:16:04] SPEAKER_01: The Ultra Ethernet is also there.
[00:16:07] SPEAKER_01: It has an alternative to RDMA, right?
[00:16:11] SPEAKER_01: It's a remote memory access that has reliable ordered delivery, reliable unordered delivery.
[00:16:19] SPEAKER_01: unreliable, unordered delivery.
[00:16:21] SPEAKER_01: So it has those specific like protocol delivery sublayers and then a semantic sublayer along with the receiver congestion control.
[00:16:34] SPEAKER_01: So I think that that makes the progress.
[00:16:38] SPEAKER_01: And so everyone will use UltraEthernet as you want to scale higher number of nodes for inference.
[00:16:48] SPEAKER_00: For inference, do you think you need very dense local interconnects, or you need a super node?
[00:17:01] SPEAKER_00: And also, I'm curious about the interconnect.
[00:17:07] SPEAKER_00: Do you prefer copper cabling, or do you prefer optic transceivers?
[00:17:16] SPEAKER_01: Yeah, the inference prefers the high-density local interconnect.
[00:17:21] SPEAKER_01: And then, you know, you can see from NVIDIA Blackwell, their rack, that everyone wants to do copper for lower cost and lower power.
[00:17:32] SPEAKER_00: Right, right.
[00:17:34] SPEAKER_00: And also, like, GB200, like, NVIDIA uses copper cabling.
[00:17:39] SPEAKER_00: Yeah, exact copper cable, correct.
[00:17:42] SPEAKER_00: So do you feel like the copper cable would be like a main solution, like maybe in the two or three years?
[00:17:50] SPEAKER_01: Yeah, I believe so.
[00:17:51] SPEAKER_01: I mean, you still have transceivers for longer distance, right?
[00:17:55] SPEAKER_01: You're going to have, like, there's no copper cable that's going to run for 100 meters, right?
[00:17:59] SPEAKER_01: This is all for like five meters.
[00:18:02] SPEAKER_01: These are active electrical cables, right?
[00:18:04] SPEAKER_01: It's like zero to five meters, zero to three meter, and not even five meters, probably three meters.
[00:18:10] SPEAKER_01: But that's what you want.
[00:18:11] SPEAKER_01: You want high density, local interconnect.
[00:18:13] SPEAKER_01: You're not trying to connect 500 meters, everything.
[00:18:17] SPEAKER_00: Right.
[00:18:19] SPEAKER_00: And also, as we see.
[00:18:22] SPEAKER_00: There's a rumor about NVIDIA's roadmap that it's pushing towards like the 200, 280 GPUs per rack and drawing one megawatts per rack.
[00:18:35] SPEAKER_00: Maybe that is for Ruben.
[00:18:38] SPEAKER_01: That's the Ruben, yeah.
[00:18:39] SPEAKER_01: That's why you saw the Mount Diablo if you went to OCP that Meta and Microsoft are working on.
[00:18:48] SPEAKER_01: that is supposed to support, it's today 200 kilowatt, it's supposed to scale to one megawatt for that reason.
[00:18:57] SPEAKER_01: They want to do immersion cooling in the future and put that many GPU per rack, yeah.
[00:19:02] SPEAKER_00: Yeah, so that will like, the moving towards the effects, you call dense local interconnects.
[00:19:09] SPEAKER_00: And so like most of the connection will just happen inside the rack, right?
[00:19:15] SPEAKER_00: So do you think that could be negative to the total market, like adjustable market for transceivers?
[00:19:23] SPEAKER_00: Do you view that as a negative factor?
[00:19:28] SPEAKER_01: But you still need to connect the top-of-rack switch to top-of-rack switch, and there's more number of GPUs.
[00:19:34] SPEAKER_01: So see, I mean, I don't think you're going to see like
[00:19:39] SPEAKER_01: You know, how many people actually, you're going to see the network switch of the one, you're going to need 1.6T and 3.2T, you're going to have like linear drive and co-packaged optics, right?
[00:19:49] SPEAKER_01: So I do think the optical transceiver market is going to go down, I agree regardless.
[00:19:56] SPEAKER_00: Okay, so you do believe optical transceiver market is going down?
[00:20:01] SPEAKER_01: Yeah, I think there's no LRA
[00:20:04] SPEAKER_01: there's no 2.5 kilometer 800G, right?
[00:20:07] SPEAKER_01: There's only, or sorry, the 10 kilometer, there's only the DR8s and the FR8s, right?
[00:20:15] SPEAKER_01: The DR8 is 500 meters, the FR8 is 2.5 kilometers, and even the 2.5 kilometer on 800G is much less than on 400G.
[00:20:25] SPEAKER_01: And you're looking at linear optics, sorry, linear drive, and then the future co-packaged optics,
[00:20:31] SPEAKER_01: So then, you know, what we're seeing here is that clearly the optical transceiver is not, you know, there's not that much growth.
[00:20:39] SPEAKER_01: Everyone is trying to cut the power and cut the cost.
[00:20:43] SPEAKER_01: You're still going to have some, like, they're trying to integrate as much of that into the switch as possible.
[00:20:48] SPEAKER_01: So what you have on the transceiver just may be the laser and the TIA and the photodiode.
[00:20:54] SPEAKER_01: So yeah, the revenue of transceivers goes down with time.
[00:21:00] SPEAKER_00: I see.
[00:21:01] SPEAKER_00: That's interesting.
[00:21:06] SPEAKER_00: You just mentioned the code patch.
[00:21:07] SPEAKER_00: Can you elaborate more on that?
[00:21:12] SPEAKER_01: Yeah, co-packaged optics means that you're going to have an optical engine that is going to be like part of the switch ASIC.
[00:21:18] SPEAKER_01: It's going to be co-packaged with that in the same package, such that you're going to have like a pigtail, like a fiber coming in.
[00:21:25] SPEAKER_01: The laser may be external, the TIA, the photodiode, but, you know, like taking that optical engine and then basically recovering that signal such that
[00:21:41] SPEAKER_01: you're going to be doing like not only the DSP, but you could do like even integrated laser in the future.
[00:21:52] SPEAKER_01: So it requires like some proprietary connector.
[00:21:57] SPEAKER_01: So you're trying to do single package substrate, where I'm doing optical IO to the switch motherboard.
[00:22:05] SPEAKER_01: And then
[00:22:07] SPEAKER_01: The long goal is to put the laser also into the switch, but I'm not sure how practical that can be.
[00:22:15] SPEAKER_01: But they may still just say, okay, we're going to drive, we're going to have the fiber and coupling connector come out with external, like that transceiver is going to be plugged into this switch package with the laser TIA diode.
[00:22:30] SPEAKER_01: But basically coming out of the board, you're going to see the fiber coming in.
[00:22:35] SPEAKER_01: So this is going to be needed because as you go to higher and higher bandwidth, the loss of the copper on the routing on the PCB is challenging, right?
[00:22:46] SPEAKER_01: And so is the power delivery.
[00:22:57] SPEAKER_00: So as you mentioned, now the data center, the compute is transitioning from the chip level advances to board level, maybe system level integration.
[00:23:12] SPEAKER_00: So how do you view this affects competitive landscape over different vendors for GPU, like NVIDIA, AMD, maybe ASICs?
[00:23:23] SPEAKER_00: and not like TPU, or maybe can you talk about Microsoft's homegrown ASIC, like Maya, maybe, can you talk about which one, who will be the long-time winner?
[00:23:42] SPEAKER_01: Yeah.
[00:23:43] SPEAKER_01: I think clearly Google TPU is the highest
[00:23:49] SPEAKER_01: you know, that is there, right?
[00:23:51] SPEAKER_01: For as the most farthest ahead in competing with the NVIDIA and AMD, right?
[00:24:00] SPEAKER_01: You know, AWS is behind Google.
[00:24:02] SPEAKER_01: You know, you have different AI engine vector coarse grain reconfigurable arrays, you know, like the parallel dispatch
[00:24:14] SPEAKER_01: The tile, you can see at the hot chips, they showed the architecture of my 16 cluster of tile and each tile has four sub-block with the core processor in each tile along with a scalar and vector engine and DMA and a tensor core.
[00:24:35] SPEAKER_01: I do think Microsoft continues to invest in development.
[00:24:39] SPEAKER_01: I think everyone in this today is developing the vector engine and, you know, trying to accelerate like the ReLU, the low-level primitive functions of the DNN, basically of the machine learning, to see how to best get the ML performance, to increase efficiency usage across layers within the model.
[00:25:02] SPEAKER_01: I do think though that NVIDIA, Microsoft will be heavily reliant on NVIDIA and AMD till 2029 for sure.
[00:25:11] SPEAKER_01: Like if you guys look at the market share, like deployed data center share of AI chip and GPU chip, the Maya in 2029 I think is still like not even gonna be 20, it's gonna be probably 10 to 12%.
[00:25:24] SPEAKER_01: You know, the Maya 300, let's just say.
[00:25:29] SPEAKER_01: NVIDIA GPUs will probably be 65% and the rest is going to be AMD.
[00:25:33] SPEAKER_01: 10% for AMD?
[00:25:33] SPEAKER_01: No, 65% NVIDIA, Maya 12%, the rest AMD.
[00:25:38] SPEAKER_00: That is for 2025?
[00:25:50] SPEAKER_01: No, that's for 2029.
[00:25:51] SPEAKER_00: 2029, okay.
[00:25:51] SPEAKER_00: No, 2025 is like less than 2% Maya.
[00:25:53] SPEAKER_01: It should be like probably 80% NVIDIA and then 18% AMD.
[00:25:55] SPEAKER_01: What is the total, total compute demand?
[00:25:58] SPEAKER_01: For 2025, it's 800K B200s.
[00:26:17] SPEAKER_01: uh... not buying any g uh... the h two hundred is all this year that taking that next year there's no h one hundred either uh... actually they may they may take a hundred eighty two hundred fifty k h two hundred next year so the two hundred k h two hundred eight hundred k b two hundreds and then it's going to be two hundred k mi three twenty five x no more mi three hundred x
[00:26:51] SPEAKER_00: Thanks for your information.
[00:26:52] SPEAKER_00: And I want to have a follow-up.
[00:26:56] SPEAKER_00: The 800K for GB200, or there's also HGXB200, like the 8 GPU version, or it's all for like 72 GPU version?
[00:27:11] SPEAKER_01: No, you're talking about the actual rack.
[00:27:14] SPEAKER_01: So they killed the, there's no NBL36.
[00:27:16] SPEAKER_01: They killed that product.
[00:27:21] SPEAKER_01: Okay, so NVIDIA killed that product.
[00:27:23] SPEAKER_01: And Microsoft gets a custom rack made with Honhai Precision.
[00:27:27] SPEAKER_01: They have a separate power cabinet and an AI server cabinet.
[00:27:31] SPEAKER_01: That's the Mount Diablo collaboration they have with Meta on the power cabinet.
[00:27:37] SPEAKER_01: Now, if you want, like, look at the
[00:27:43] SPEAKER_01: Yeah, if you want to look at the accelerator deployment, Microsoft is not, the GB200 is the one with Grace Hopper that manages, the Grace Hopper CPU manages the B200 GPU.
[00:28:01] SPEAKER_01: That's probably only like a hundred
[00:28:05] SPEAKER_01: worth of demand, the 800K.
[00:28:08] SPEAKER_01: It's not a high percentage.
[00:28:09] SPEAKER_01: So most of it is with B200, like the DGX-B200, that Microsoft will go to ODM.
[00:28:17] SPEAKER_01: So it's with the Intel CPU managing the B200 GPU.
[00:28:23] SPEAKER_01: So that's, it's a different configuration.
[00:28:29] SPEAKER_00: 100k for GP200 and 700k for for HGX or DGX B200.
[00:28:36] SPEAKER_01: Yeah, correct.
[00:28:40] SPEAKER_00: The B200, like the HGX one is like 8 GPU version or it's also... It's 8 GPUs and 2 CPUs.
[00:28:49] SPEAKER_01: 2 Intel CPUs, 8 GPUs.
[00:28:53] SPEAKER_00: I see.
[00:28:54] SPEAKER_00: I see.
[00:28:56] SPEAKER_00: Why, um...
[00:28:58] SPEAKER_00: The mix is interesting.
[00:29:00] SPEAKER_00: NVIDIA is saying GB200 is 3 or 4x faster in power efficiency.
[00:29:09] SPEAKER_00: That's not the case.
[00:29:11] SPEAKER_01: The Grace Hopper is not that much better.
[00:29:14] SPEAKER_01: It's actually an inferior CPU to the Intel.
[00:29:18] SPEAKER_01: Because ARM has their ISA and their PPA and such is always going to be behind x86.
[00:29:26] SPEAKER_01: Because x86, you can do multi-threading, and then also you can get a higher clock frequency on x86, and you can execute instructions in memory, right?
[00:29:35] SPEAKER_01: In the x86 architecture, you cannot... In the CISC architecture, in the RISC architecture, you don't have complex operands.
[00:29:42] SPEAKER_01: You can't execute in memory.
[00:29:44] SPEAKER_01: You have to do everything in a register.
[00:29:48] SPEAKER_01: Now, you can get better power consumption, but you're never going to get better performance.
[00:29:54] SPEAKER_01: And then, you know, the other thing you're going to see is that, you know, the IO coherency.
[00:30:00] SPEAKER_01: So if you do the Grace Hopper, you know, they claim that you're going to have better coherency management between the CPU and the GPU, but you can get the same if you do this over PCIe.
[00:30:10] SPEAKER_01: It's an IO coherent bus.
[00:30:12] SPEAKER_01: You do the A-slide, invalidation, the stashing, the right ones, right line unique, et cetera, operations.
[00:30:21] SPEAKER_01: So I think what you're going to find is that
[00:30:24] SPEAKER_01: If you just rewrite the PCIe driver in the Linux kernel and you modify this and you get it to work with the Intel CPU, you're going to get equivalent performance to the actual Grace Hopper in the cache coherency.
[00:30:37] SPEAKER_01: And the CPU itself of Intel outperforms the Grace Hopper CPU.
[00:30:45] SPEAKER_00: Right.
[00:30:45] SPEAKER_00: I understand your point, like the Intel CPU could be better than Grace, but
[00:30:55] SPEAKER_00: To put it in another way, the GP200 will integrate 72 GPUs in one rack.
[00:31:03] SPEAKER_00: So that is good for the dense local interconnect for inference, like O1, this kind of workload, correct?
[00:31:14] SPEAKER_00: Like higher batch size and lower latency?
[00:31:25] SPEAKER_01: Okay, so I don't think the latency will improve if the B200 is managed by the Grace Hopper CPU.
[00:31:32] SPEAKER_01: That doesn't, so the, if you have cache coherency, you can argue that you could do faster, so like you could say you're going to have higher throughput access, and yeah, I guess potentially you could have
[00:31:48] SPEAKER_01: lower latency for actual data sharing.
[00:31:52] SPEAKER_01: But if you rewrite the PCIe driver to do IO coherency in the Linux kernel and do the actual memory region sharing, then I think you're going to get similar performance on the x86.
[00:32:05] SPEAKER_01: You just have to rewrite the driver.
[00:32:06] SPEAKER_00: Right.
[00:32:14] SPEAKER_01: Why are not... See, automatically they talk the same protocol, right?
[00:32:18] SPEAKER_01: This is CHI, ACE-Lite, this is the same protocol that both CPUs talk externally.
[00:32:26] SPEAKER_01: The only advantage is that NVIDIA has configured and optimized the driver for you if you buy the GB200, whereas you have to do this yourself if Intel has not done this for the B200 with their CPU.
[00:32:41] SPEAKER_01: But they're both fundamentally CPUs, so I don't think that if you optimize the driver, I don't see how one gives you better latency than the other.
[00:32:52] SPEAKER_00: But you can put Intel CPU in...
[00:32:55] SPEAKER_00: like the rack, like this is an open rack, right?
[00:32:59] SPEAKER_01: So you still get- No, the server, this is a server.
[00:33:02] SPEAKER_01: I'm saying the server has an Intel CPU inside.
[00:33:05] SPEAKER_01: Two Intel CPUs, eight GPUs.
[00:33:09] SPEAKER_01: It's not just GPU racks.
[00:33:11] SPEAKER_01: There has to be CPUs somewhere.
[00:33:17] SPEAKER_00: Okay.
[00:33:19] SPEAKER_01: Even the GB200, there's two Grace Hopper CPUs and four B200 GPUs.
[00:33:28] SPEAKER_01: It's not monolithic, the CPU is not inside the GPU.
[00:33:33] SPEAKER_00: Yes, but it's integrated in a compute tree, and there's like eight compute trees on the top, and in the middle there's like, I don't know, like nine switches.
[00:33:43] SPEAKER_01: No, the same thing, the server, the same server, there's two Intel CPUs and eight GPUs.
[00:33:49] SPEAKER_01: They're part of the same server, you're just calling it a tray.
[00:33:54] SPEAKER_00: Oh, I see.
[00:33:59] SPEAKER_00: I see.
[00:34:01] SPEAKER_00: So you are saying that the server is a tray, not like a server, like a traditional server that we're talking about, like a DGX server.
[00:34:09] SPEAKER_00: But now you're saying like, maybe we will call it like a Miranda structure.
[00:34:14] SPEAKER_00: It's like a GB200 rack, but just CPU is different.
[00:34:20] SPEAKER_01: Yeah, and Microsoft is doing their own rack with a separate power cabinet and a separate IT, like, you know, AI server plus networking cabinet to scale the power infrastructure, yeah.
[00:34:30] SPEAKER_00: Okay, gotcha.
[00:34:36] SPEAKER_00: How many GPUs you bought for 2024?
[00:34:45] SPEAKER_01: Yeah, Microsoft bought
[00:34:48] SPEAKER_01: So they bought 800K H-100s and 100K H-200, 150K H-200.
[00:34:50] SPEAKER_01: Sorry, 600K H-100 and 150K H-200.
[00:35:17] SPEAKER_01: and they bought 200k mi300x.
[00:35:18] SPEAKER_01: You bought them like
[00:35:32] SPEAKER_00: Like, what percentage of this total 7,700k GPUs you bought on your own, like, you're deploying your own data center, and how many percent you just do, like, capital lease and maybe it's in like... I'm talking about the ones Microsoft buys.
[00:35:53] SPEAKER_01: This is not leasing.
[00:35:55] SPEAKER_01: They're separate.
[00:35:55] SPEAKER_01: It's leasing.
[00:35:56] SPEAKER_01: Like, the CoreWeave, they don't buy those.
[00:35:59] SPEAKER_01: That's fully leased.
[00:36:03] SPEAKER_01: And then, you know, the Oracle, the ones at Oracle that they may be leasing, that's Oracle that's buying that, not Microsoft.
[00:36:12] SPEAKER_01: The OpenAI, this includes the demand for OpenAI, by the way.
[00:36:19] SPEAKER_00: What's the allocation, like, between OpenAI and your own, like, L... 200K for H100 for OpenAI.
[00:36:24] SPEAKER_01: And then 50K H200 for OpenAI.
[00:36:35] SPEAKER_00: So about like 30% for open app.
[00:36:38] SPEAKER_01: Yeah.
[00:36:45] SPEAKER_00: And like, um, like the rest, that 70% is all for, um, a co-pilot and like inferencing.
[00:36:54] SPEAKER_00: Yeah.
[00:37:04] SPEAKER_00: How many GPUs will you buy in 2026?
[00:37:06] SPEAKER_01: 2026, I think that... Yeah, I would say...
[00:37:31] SPEAKER_01: Yeah, we have to see when the Rubin is out, right?
[00:37:37] SPEAKER_01: If that's out, I don't know if that meets 2025 year end.
[00:37:40] SPEAKER_01: And I think that's kind of aggressive.
[00:37:43] SPEAKER_01: So if it's going to be 2026 Q1, Q2, that's where the... So let's just say Rubin is out 2026 Q1, and it's probably going to be B200 is probably 150K, Rubin is probably like 700K.
[00:38:05] SPEAKER_01: Okay, and then if the Rubin is delayed, then it's probably 1.1 million of B200.
[00:38:18] SPEAKER_00: I see, about 30% year-on-year growth.
[00:38:22] SPEAKER_00: Yeah.
[00:38:30] SPEAKER_00: When will you receive the first
[00:38:36] SPEAKER_00: B200, will it be like November, December, or January?
[00:38:50] SPEAKER_01: Sorry, when will what be ready?
[00:38:52] SPEAKER_00: When will you deploy your first batch of B200?
[00:38:58] SPEAKER_01: B200 will be deployed actually in December, the first batch.
[00:39:03] SPEAKER_01: How big is the cluster?
[00:39:04] SPEAKER_01: So this is, when you say it deployed, you mean available on Azure, right?
[00:39:08] SPEAKER_01: Because they already have B200 today.
[00:39:11] SPEAKER_01: Or you mean B200, the DGX B200, not the GB200, is that right?
[00:39:19] SPEAKER_00: Yeah.
[00:39:20] SPEAKER_01: Okay, yeah, the DGX B200 will be January, yeah.
[00:39:25] SPEAKER_00: January, okay.
[00:39:26] SPEAKER_01: Probably like the very first deployments will be 20K.
[00:39:32] SPEAKER_00: 20K GPU or 20K GPU, right?
[00:39:34] SPEAKER_01: GPU, GPU, not server.
[00:39:36] SPEAKER_00: Okay.
[00:39:42] SPEAKER_00: Can I assume Microsoft will account for 50% of the total deployment for DGX-P200?
[00:39:50] SPEAKER_00: Yeah.
[00:39:57] SPEAKER_00: I see.
[00:39:57] SPEAKER_01: The co-location providers will buy the GB200.
[00:40:01] SPEAKER_01: Same with the GPU as a service.
[00:40:03] SPEAKER_01: They're not going to buy the DGX.
[00:40:07] SPEAKER_01: They're not going to rewrite the PCIe driver in the Linux kernel.
[00:40:14] SPEAKER_00: AWS?
[00:40:15] SPEAKER_00: Do you think AWS will buy the DGX?
[00:40:17] SPEAKER_01: AWS will buy the DGX.
[00:40:19] SPEAKER_01: They're not going to buy the GB.
[00:40:20] SPEAKER_00: What about Meta?
[00:40:25] SPEAKER_01: Meta will probably buy the DGX.
[00:40:32] SPEAKER_01: So the hyperscaler doesn't want to get completely locked into NVIDIA, so they buy the DGX.
[00:40:43] SPEAKER_00: I'm assuming AWS will buy 600k for B200 in 2025.
[00:40:46] SPEAKER_00: Yep.
[00:40:55] SPEAKER_00: Do you think that's correct?
[00:40:56] SPEAKER_00: That's correct.
[00:41:02] SPEAKER_00: And also I'm assuming Metis will buy 660k for P200.
[00:41:07] SPEAKER_00: That's correct.
[00:41:17] SPEAKER_00: What about Google?
[00:41:20] SPEAKER_01: Google will probably buy just like 200k.
[00:41:22] SPEAKER_01: They're focusing on their TPUs.
[00:41:30] SPEAKER_00: I see.
[00:41:31] SPEAKER_00: What about Oracle?
[00:41:33] SPEAKER_00: Oracle is probably 500k.
[00:41:36] SPEAKER_00: 500k.
[00:41:37] SPEAKER_00: That's a lot.
[00:41:41] SPEAKER_00: I don't think they can buy that much because they just got... That's what they want.
[00:41:46] SPEAKER_01: I don't think they're going to get... That's what they're putting in as a request.
[00:41:49] SPEAKER_01: I think ultimately they'll be around 300k.
[00:41:50] SPEAKER_01: They'll get half.
[00:41:52] SPEAKER_00: That makes sense.
[00:41:57] SPEAKER_00: What about XAI and Tesla?
[00:42:00] SPEAKER_01: I think Tesla is going to be good for like 400k.
[00:42:05] SPEAKER_01: Right, right.
[00:42:11] SPEAKER_00: For Microsoft, like the last quarter and this quarter, you have like about, um, let me check, 6 billion, um, 6 billion like capitalists.
[00:42:24] SPEAKER_00: Let me see.
[00:42:27] SPEAKER_00: So will you have that much capital lease in the next year?
[00:42:35] SPEAKER_01: So the leasing decreases going forward.
[00:42:40] SPEAKER_00: Oh, okay.
[00:42:42] SPEAKER_00: Makes sense.
[00:42:55] SPEAKER_00: What about the infrastructure costs?
[00:42:57] SPEAKER_00: Because now you are spending like 50% of the cash capex in lending and infrastructure.
[00:43:04] SPEAKER_00: What would this structure involve?
[00:43:06] SPEAKER_01: No, but I think that the trend is going to be for like the built-to-suite or single-tenant COLO.
[00:43:13] SPEAKER_01: I don't know how many more data centers Microsoft does.
[00:43:16] SPEAKER_01: I think in the US, like in West Texas, like where land is cheap, they can do the
[00:43:21] SPEAKER_01: the owned and operated but abroad and other areas are like high density metro you may think it's too expensive to do the by the land and too much time you may probably do the built to suite or the single tenant column.
[00:43:49] SPEAKER_00: How many GPUs do you think NVIDIA will ship in 2025?
[00:44:03] SPEAKER_01: You think how many total GB200s and B200s?
[00:44:07] SPEAKER_01: Just the GPU, not the... So the total number of B200s including GB200 and B200, right?
[00:44:13] SPEAKER_00: Right.
[00:44:14] SPEAKER_01: Yeah, I think probably 2.1 million.
[00:44:17] SPEAKER_00: 2.1 million, okay.
[00:44:19] SPEAKER_00: So how many copper, H200, do you think?
[00:44:24] SPEAKER_01: Actually, B200 is probably 2.2 million, 2.3 million.
[00:44:25] SPEAKER_01: The H200, probably like 1.2 million.
[00:44:27] SPEAKER_01: So total 3.6 million?
[00:44:28] SPEAKER_01: Yeah.
[00:44:28] SPEAKER_01: That's well below consensus.
[00:44:44] SPEAKER_00: Now the market is estimating NVIDIA to ship 5 million GPUs.
[00:44:53] SPEAKER_01: I think the first two quarters will be strong, but you're going to see maybe some pullback in the latter half of the year unless they have very robust demand from the applications.
[00:45:06] SPEAKER_00: Why is that?
[00:45:09] SPEAKER_01: you're going to have to see that the increase, the AI usage rate has to go up.
[00:45:14] SPEAKER_01: Right now, about 33% of the employee uses the AI.
[00:45:21] SPEAKER_01: And then, you know, that number has to go up, and then, you know, other enterprise adoption rate.
[00:45:29] SPEAKER_01: Five million GPU, I think, I think it's, hmm,
[00:45:37] SPEAKER_01: Yeah, I mean, I think I can definitely see 4 million, maybe the, okay, so, so you know what, actually the hyperscaler is 2 point, like, something million, probably the, okay, maybe the Blackwell can be 3 million, 3.2 million, okay?
[00:45:53] SPEAKER_01: And then the H200 can be 1.3 or 1.4 million, but I don't, I don't think they, like, 4.4, 4.3 million, I thought that that's going to be the higher end of the spectrum.
[00:46:07] SPEAKER_01: I don't know about 5 million, that seems quite high.
[00:46:13] SPEAKER_00: I see.
[00:46:14] SPEAKER_00: So the number you just shared is just like Hyperscaler, you don't take into account the enterprise?
[00:46:23] SPEAKER_01: Yeah, I don't take into account the other customers.
[00:46:27] SPEAKER_00: I see.
[00:46:27] SPEAKER_00: So I think Hyperscaler accounts for 60% of the total market?
[00:46:33] SPEAKER_01: Yeah, about right.
[00:46:39] SPEAKER_00: Let's go back to the LLM question.
[00:46:42] SPEAKER_00: I'm also curious about the GPT-5.
[00:46:47] SPEAKER_00: Do you have any news on that?
[00:46:50] SPEAKER_01: Yeah, that OpenAI is more prioritizing the O, right?
[00:46:55] SPEAKER_01: I think if the O does not get ready by June or July next year, they'll release GPT-5.
[00:47:02] SPEAKER_01: Like May.
[00:47:10] SPEAKER_00: What is the challenge for GPT-5?
[00:47:13] SPEAKER_01: Well, they need to show better accuracy, right?
[00:47:16] SPEAKER_01: You know, you get an accuracy on the high-end applications of, say, 96%.
[00:47:21] SPEAKER_01: But that's not good enough.
[00:47:23] SPEAKER_01: You need to be at like 99.2% to be able to replace the human.
[00:47:29] SPEAKER_00: Mm-hmm.
[00:47:30] SPEAKER_00: Correct.
[00:47:38] SPEAKER_00: So OpenAI
[00:47:40] SPEAKER_00: like prioritizing O1 and will release O1 maybe in the first quarter of 2025, the official version of O1, and then maybe May or June will be GPT-5.
[00:47:54] SPEAKER_00: Is that correct?
[00:48:03] SPEAKER_01: Yeah.
[00:48:04] SPEAKER_01: They won't release O1 until it's ready.
[00:48:06] SPEAKER_01: If they release it and it fails, it won't do much.
[00:48:12] SPEAKER_00: Yes.
[00:48:16] SPEAKER_00: You know what?
[00:48:18] SPEAKER_00: Recently, Enthopic just upgraded the SONET 3.5, right?
[00:48:23] SPEAKER_00: But they did not upgrade SONET Altus, which is the biggest version, the highest parameter count version.
[00:48:33] SPEAKER_00: And also, we talked to Gemini expert, and he
[00:48:39] SPEAKER_00: He said they will not do a bigger version of Gemini.
[00:48:43] SPEAKER_00: So this gave me some concern about hitting walls in pre-training.
[00:48:54] SPEAKER_00: And maybe there's no, yeah.
[00:49:00] SPEAKER_00: Could you share your perspective on the diminishing returns curve in pre-training?
[00:49:15] SPEAKER_01: Oh.
[00:49:22] SPEAKER_01: Yeah, so in terms of the pre-training, I think that... No, no, but you're saying diminishing returns of pre-training.
[00:49:37] SPEAKER_01: I agree with you that if you want to achieve higher and higher accuracy for specific, you have to do reinforcement learning.
[00:49:46] SPEAKER_01: Oh, you have increased parameter, the pre training, and you know, like the, the fine tuning can only get you to the 9590 probably like 97% 98%.
[00:49:56] SPEAKER_01: You have to do reinforcement learning to get the 99.2 99.3.
[00:49:58] SPEAKER_01: Right.
[00:50:17] SPEAKER_00: And with O1s demonstrating that longer inference times that can yield better results and better accuracy, mostly in areas like coding and math, how will you adapt to infrastructure cost models?
[00:50:40] SPEAKER_00: How will you approach to balancing inference quality versus latency?
[00:50:48] SPEAKER_01: Yeah, I think the latency is determined by the GPU and the network, right?
[00:50:53] SPEAKER_01: The configuration more so than the model.
[00:50:56] SPEAKER_01: There could be a time-to-first token, but that's determined by hardware, how much on-chip SRAM, etc., that you have.
[00:51:04] SPEAKER_01: In terms of, like, the accuracy of the model,
[00:51:11] SPEAKER_01: I think that one is determined by, you can only achieve some limit with the fine-tuning.
[00:51:17] SPEAKER_01: You have to do reinforcement learning to get higher accuracy.
[00:51:29] SPEAKER_00: Thank you.
[00:51:30] SPEAKER_00: And now we have about 10 minutes for audience questions.
[00:51:40] SPEAKER_00: audience can feel free to put your questions in the chat, and I will translate the Chinese to English.
[00:51:51] SPEAKER_00: And we got the first question.
[00:51:57] SPEAKER_00: That's a good question.
[00:52:00] SPEAKER_00: He's asking what's your view on Vera, NVIDIA's GPU Vera, which uses NVIDIA's self-developed microarchitecture, not ARM version two.
[00:52:16] SPEAKER_00: Is it expected to catch up with x86 performance?
[00:52:23] SPEAKER_01: No, I don't believe so.
[00:52:25] SPEAKER_01: I think they want to do their own ISA for CPU.
[00:52:29] SPEAKER_01: See, OK, you either have to do ARM RISC-V or you have to do one of those.
[00:52:34] SPEAKER_01: I think that the CPU side, they're better off continuing the ARM path for the hyper... No one in RISC-V has come close to Neoverse v2, let alone v3.
[00:52:47] SPEAKER_01: There are people who make clean like Akiyana.
[00:52:50] SPEAKER_01: NVIDIA is better off focusing on GPU than doing CPUs.
[00:52:59] SPEAKER_01: So many people are trying Cy5.
[00:53:02] SPEAKER_01: You have the Andes.
[00:53:04] SPEAKER_01: You have like the Akiyana.
[00:53:06] SPEAKER_01: You have the Tense Torrent.
[00:53:08] SPEAKER_01: No one is matching x86 performance.
[00:53:12] SPEAKER_01: Let alone even on the ARM performance.
[00:53:21] SPEAKER_00: The next question is about ASIC roadmap.
[00:53:25] SPEAKER_00: What's Maya and Cobalt's technology roadmap and timeline in terms of process nodes and advanced packaging?
[00:53:34] SPEAKER_01: Yeah, so those are both 5 nanometer.
[00:53:37] SPEAKER_01: And then, see, the Maya is like HBM2.
[00:53:41] SPEAKER_01: The Maya 200 will be HBM4.
[00:53:45] SPEAKER_01: either via the HBM4 or HBM3E.
[00:53:48] SPEAKER_01: They're both, so the Cobalt is ARM-based CPU, right?
[00:53:52] SPEAKER_01: They're gonna continue scaling with regular 2.5D organic substrate package.
[00:53:59] SPEAKER_01: There's no HBM or anything, right?
[00:54:03] SPEAKER_01: This is not even chiplet.
[00:54:06] SPEAKER_01: So I think that the,
[00:54:10] SPEAKER_01: that one there's no I think even the next generation they may do on five nanometer the cobalt 200 there's no advantage to go to three I think the Maya 300 or 200 has to be done on four nanometer to get better performance and maybe even eventually the bias like 2028 they're going to be on three nanometer they're going to be on HBM for core so
[00:54:36] SPEAKER_01: In terms of process node, yeah, the Maya is going to keep going down from 5, 4, 3.
[00:54:43] SPEAKER_01: Cobalt 200 will be 5.
[00:54:45] SPEAKER_01: I think by 2028, they'll be on 4 nanometer.
[00:54:49] SPEAKER_01: I don't think they need to go to 3 nanometer to do the cobalt.
[00:54:52] SPEAKER_01: I don't think it gives them much benefit.
[00:55:01] SPEAKER_00: That's helpful content, thank you, that's helpful.
[00:55:04] SPEAKER_00: And the next question is about GPU order volume.
[00:55:11] SPEAKER_00: He's asking how many B300 Microsoft will buy in 2025.
[00:55:13] SPEAKER_00: Sorry, the B300?
[00:55:28] SPEAKER_00: Yes, which is the B200 Ultra, and now NVIDIA is naming it B300.
[00:55:32] SPEAKER_00: Oh, okay, yeah, yeah, yeah.
[00:55:35] SPEAKER_00: It's HBM, like HBM 12 high version.
[00:55:38] SPEAKER_01: Yeah, yeah, yeah, the HBM, no, no, but all the HBM, it's just all HBM 3E, it's all 12 high, right?
[00:55:47] SPEAKER_01: You have 12 high and 16 high.
[00:55:49] SPEAKER_00: Yeah, yeah, yeah, right.
[00:55:53] SPEAKER_01: Yeah, the B300 will be likely ready in, like, June.
[00:55:58] SPEAKER_01: I think if that's the case, they'll convert the whatever rest of the year demand of B200 to B300.
[00:56:05] SPEAKER_01: So maybe the maximum they can do is like some 700K.
[00:56:08] SPEAKER_01: Sorry, sorry, sorry.
[00:56:11] SPEAKER_01: Yeah, it'll be all the DGX variant.
[00:56:15] SPEAKER_01: So 700K is the total.
[00:56:18] SPEAKER_01: And then probably by June, that means it's already like about halfway to year end.
[00:56:24] SPEAKER_01: So they can order 400K.
[00:56:29] SPEAKER_00: Right, okay.
[00:56:32] SPEAKER_00: That's helpful.
[00:56:34] SPEAKER_00: So the total would be like 700K and the allocation depends on the product, right?
[00:56:41] SPEAKER_00: When were the product ready?
[00:56:45] SPEAKER_01: Correct.
[00:56:49] SPEAKER_00: And also, he's asking about AMD.
[00:56:53] SPEAKER_00: What is the order volume for AMD in 2024, 2025, maybe 2026?
[00:57:00] SPEAKER_01: Yeah, the 2025 is going to be, sorry, 2024 it was 220K, right, of MI 300X.
[00:57:04] SPEAKER_01: 2025 is, it's all the, it's all gonna be the, yeah,
[00:57:24] SPEAKER_01: Yeah, it's all gonna be MI325.
[00:57:28] SPEAKER_01: The idea is that, see, H200 price comes down, so you don't wanna keep buying MI300X, you'd rather buy H200.
[00:57:35] SPEAKER_01: So the MI325X will be 20.25 demand, and that's gonna be like 190K worth.
[00:57:37] SPEAKER_01: And then, yeah, so, and then, yeah.
[00:57:55] SPEAKER_01: Yeah.
[00:57:59] SPEAKER_01: Yeah, and I'm sorry, just to correct, so you're right that the B200 and GB200 is 8H.
[00:58:06] SPEAKER_01: It's 192 gig of HBM3, the B300, GB300 is the 288, yeah.
[00:58:11] SPEAKER_01: That's likely going to be ready by June.
[00:58:14] SPEAKER_01: That means Microsoft takes delivery of those from June.
[00:58:18] SPEAKER_01: NVIDIA will have ready just after Chinese New Year, like March.
[00:58:24] SPEAKER_01: And then they have to ramp up the capacity.
[00:58:29] SPEAKER_00: I see.
[00:58:30] SPEAKER_00: I see.
[00:58:30] SPEAKER_00: That's helpful.
[00:58:35] SPEAKER_00: We are running out of time.
[00:58:36] SPEAKER_00: Let's take one more question from the audience before we wrap up.
[00:58:48] SPEAKER_00: This one is good.
[00:58:49] SPEAKER_00: Do you believe post-training also follows scaling law?
[00:58:54] SPEAKER_00: just like pre-training before?
[00:58:57] SPEAKER_00: And is reinforcement learning the way to lift the model performance saving?
[00:59:08] SPEAKER_01: I think so, yes.
[00:59:10] SPEAKER_01: I think post-training, in terms of fine-tuning, also has the limit, right?
[00:59:16] SPEAKER_01: I think reinforcement is the way to go forward, and that does increase GPU utilization.
[00:59:23] SPEAKER_01: but also allows you to limit the parameter count of the model.
[00:59:30] SPEAKER_01: The reason you're not gonna see models more than three trillion parameters is that it's more efficient to implement reinforcement learning than to add more and more parameter and do static one-time pre-training and then fine-tuning.
[00:59:50] SPEAKER_00: Thank you.
[00:59:51] SPEAKER_00: Let's wrap up now.
[00:59:52] SPEAKER_00: You give us very incredible insight.
[00:59:57] SPEAKER_00: And thank you again for sharing your expertise, and look forward to our future chats.
[01:00:04] SPEAKER_01: OK, thank you.
[01:00:05] SPEAKER_00: Thank you.
[01:00:06] SPEAKER_00: Goodbye.
